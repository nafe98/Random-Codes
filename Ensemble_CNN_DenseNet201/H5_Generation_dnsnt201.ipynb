{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5620 files belonging to 8 classes.\n",
      "Found 1730 files belonging to 8 classes.\n",
      "Found 680 files belonging to 8 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/40\n",
      "352/352 [==============================] - 136s 319ms/step - loss: 2.2755 - accuracy: 0.1835 - val_loss: 1.7253 - val_accuracy: 0.5127\n",
      "Epoch 2/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 1.8017 - accuracy: 0.2952 - val_loss: 1.2885 - val_accuracy: 0.6451\n",
      "Epoch 3/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 1.5581 - accuracy: 0.4075 - val_loss: 0.9146 - val_accuracy: 0.7185\n",
      "Epoch 4/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 1.3419 - accuracy: 0.4762 - val_loss: 0.7245 - val_accuracy: 0.7613\n",
      "Epoch 5/40\n",
      "352/352 [==============================] - 81s 232ms/step - loss: 1.1390 - accuracy: 0.5617 - val_loss: 0.5123 - val_accuracy: 0.8046\n",
      "Epoch 6/40\n",
      "352/352 [==============================] - 81s 232ms/step - loss: 0.9420 - accuracy: 0.6436 - val_loss: 0.3852 - val_accuracy: 0.8948\n",
      "Epoch 7/40\n",
      "352/352 [==============================] - 82s 233ms/step - loss: 0.8186 - accuracy: 0.6879 - val_loss: 0.2943 - val_accuracy: 0.9145\n",
      "Epoch 8/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.6613 - accuracy: 0.7587 - val_loss: 0.2460 - val_accuracy: 0.9266\n",
      "Epoch 9/40\n",
      "352/352 [==============================] - 81s 231ms/step - loss: 0.5462 - accuracy: 0.8075 - val_loss: 0.2225 - val_accuracy: 0.9306\n",
      "Epoch 10/40\n",
      "352/352 [==============================] - 81s 231ms/step - loss: 0.4275 - accuracy: 0.8498 - val_loss: 0.2379 - val_accuracy: 0.9358\n",
      "Epoch 11/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.3406 - accuracy: 0.8797 - val_loss: 0.3004 - val_accuracy: 0.9272\n",
      "Epoch 12/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.3063 - accuracy: 0.8966 - val_loss: 0.3643 - val_accuracy: 0.9266\n",
      "Epoch 13/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.2296 - accuracy: 0.9192 - val_loss: 0.3205 - val_accuracy: 0.9347\n",
      "Epoch 14/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.2014 - accuracy: 0.9310 - val_loss: 0.3654 - val_accuracy: 0.9318\n",
      "Epoch 15/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1885 - accuracy: 0.9377 - val_loss: 0.3292 - val_accuracy: 0.9341\n",
      "Epoch 16/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1731 - accuracy: 0.9459 - val_loss: 0.3448 - val_accuracy: 0.9306\n",
      "Epoch 17/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1454 - accuracy: 0.9507 - val_loss: 0.3472 - val_accuracy: 0.9434\n",
      "Epoch 18/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1393 - accuracy: 0.9559 - val_loss: 0.4038 - val_accuracy: 0.9358\n",
      "Epoch 19/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1221 - accuracy: 0.9577 - val_loss: 0.4815 - val_accuracy: 0.9312\n",
      "Epoch 20/40\n",
      "352/352 [==============================] - 83s 235ms/step - loss: 0.1217 - accuracy: 0.9623 - val_loss: 0.4660 - val_accuracy: 0.9358\n",
      "Epoch 21/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1194 - accuracy: 0.9673 - val_loss: 0.5979 - val_accuracy: 0.9249\n",
      "Epoch 22/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.1148 - accuracy: 0.9646 - val_loss: 0.3897 - val_accuracy: 0.9451\n",
      "Epoch 23/40\n",
      "352/352 [==============================] - 81s 232ms/step - loss: 0.0949 - accuracy: 0.9678 - val_loss: 0.4190 - val_accuracy: 0.9428\n",
      "Epoch 24/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0746 - accuracy: 0.9758 - val_loss: 0.5027 - val_accuracy: 0.9393\n",
      "Epoch 25/40\n",
      "352/352 [==============================] - 82s 233ms/step - loss: 0.0893 - accuracy: 0.9738 - val_loss: 0.5672 - val_accuracy: 0.9358\n",
      "Epoch 26/40\n",
      "352/352 [==============================] - 82s 234ms/step - loss: 0.0739 - accuracy: 0.9760 - val_loss: 0.6306 - val_accuracy: 0.9387\n",
      "Epoch 27/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0913 - accuracy: 0.9738 - val_loss: 1.1605 - val_accuracy: 0.8688\n",
      "Epoch 28/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0748 - accuracy: 0.9776 - val_loss: 0.5130 - val_accuracy: 0.9405\n",
      "Epoch 29/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.4830 - val_accuracy: 0.9428\n",
      "Epoch 30/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0527 - accuracy: 0.9827 - val_loss: 0.6177 - val_accuracy: 0.9393\n",
      "Epoch 31/40\n",
      "352/352 [==============================] - 83s 236ms/step - loss: 0.0670 - accuracy: 0.9804 - val_loss: 0.5539 - val_accuracy: 0.9410\n",
      "Epoch 32/40\n",
      "352/352 [==============================] - 81s 231ms/step - loss: 0.0537 - accuracy: 0.9840 - val_loss: 0.6282 - val_accuracy: 0.9387\n",
      "Epoch 33/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0450 - accuracy: 0.9865 - val_loss: 0.5818 - val_accuracy: 0.9410\n",
      "Epoch 34/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0502 - accuracy: 0.9843 - val_loss: 0.5798 - val_accuracy: 0.9428\n",
      "Epoch 35/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0607 - accuracy: 0.9838 - val_loss: 0.5530 - val_accuracy: 0.9457\n",
      "Epoch 36/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0465 - accuracy: 0.9895 - val_loss: 0.5997 - val_accuracy: 0.9468\n",
      "Epoch 37/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0464 - accuracy: 0.9881 - val_loss: 0.7050 - val_accuracy: 0.9422\n",
      "Epoch 38/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0426 - accuracy: 0.9875 - val_loss: 0.6082 - val_accuracy: 0.9422\n",
      "Epoch 39/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0479 - accuracy: 0.9872 - val_loss: 0.6463 - val_accuracy: 0.9329\n",
      "Epoch 40/40\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 0.0575 - accuracy: 0.9845 - val_loss: 0.6679 - val_accuracy: 0.9468\n",
      "43/43 [==============================] - 8s 134ms/step - loss: 0.8706 - accuracy: 0.9382\n",
      "43/43 [==============================] - 5s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "\n",
    "# Directories for training, validation, and test data\n",
    "train_dir = 'train'\n",
    "val_dir = 'validation'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Image size and batch size\n",
    "img_size = (224, 224)  # Changed to 224x224 for DenseNet compatibility\n",
    "batch_size = 16  # Reduced batch size to fit in memory\n",
    "\n",
    "# Load datasets\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Data normalization function\n",
    "def norm_data(ds):\n",
    "    prepr_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    norm_ds = ds.map(lambda x, y: (prepr_layer(x), y))\n",
    "    return norm_ds\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_data1(ds, crop_size):\n",
    "    layer1 = tf.keras.layers.CenterCrop(crop_size, crop_size)\n",
    "    aug_ds = ds.map(lambda x, y: (layer1(x), y))\n",
    "\n",
    "    layer2 = tf.keras.layers.RandomRotation(factor=(-0.05, 0.05))\n",
    "    aug_ds = aug_ds.map(lambda x, y: (layer2(x), y))\n",
    "\n",
    "    layer3 = tf.keras.layers.RandomZoom(height_factor=(-0.1, -0.01), width_factor=(-0.1, -0.01))\n",
    "    aug_ds = aug_ds.map(lambda x, y: (layer3(x), y))\n",
    "\n",
    "    return aug_ds\n",
    "\n",
    "# Image height for augmentation\n",
    "img_height = 244\n",
    "\n",
    "# Augment and normalize datasets\n",
    "train_ds_aug = augment_data1(train_ds, img_height - 20)\n",
    "train_ds_norm = norm_data(train_ds_aug)\n",
    "\n",
    "val_ds_aug = augment_data1(val_ds, img_height - 20)\n",
    "val_ds_norm = norm_data(val_ds_aug)\n",
    "\n",
    "test_ds_aug = augment_data1(test_ds, img_height - 20)\n",
    "test_ds_norm = norm_data(test_ds_aug)\n",
    "\n",
    "# Autotune for performance optimization\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds_norm = train_ds_norm.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_norm = val_ds_norm.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds_norm = test_ds_norm.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Load DenseNet-201 model without classifier layers\n",
    "base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Set last 200 layers to be trainable\n",
    "for layer in base_model.layers[-200:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "flat = Flatten()(base_model.output)\n",
    "drop1 = Dropout(0.4)(flat)\n",
    "class1 = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-8))(drop1)\n",
    "drop2 = Dropout(0.4)(class1)\n",
    "class2 = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-8))(drop2)\n",
    "drop3 = Dropout(0.4)(class2)\n",
    "class3 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-8))(drop3)\n",
    "drop4 = Dropout(0.4)(class3)\n",
    "class4 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-8))(drop4)\n",
    "drop5 = Dropout(0.4)(class4)\n",
    "predictions = Dense(8, activation='softmax', kernel_regularizer=regularizers.l2(1e-8))(drop5)\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the normalized training dataset and validate on validation dataset\n",
    "history = model.fit(train_ds_norm, epochs=40, validation_data=val_ds_norm)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_ds_norm)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred = model.predict(test_ds_norm)\n",
    "\n",
    "# Save the trained model in h5 format\n",
    "model.save(\"densenet201.h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1zR3_U1rWNAzDUIzhXt9Nf51A6CsIPB13",
     "timestamp": 1702576649592
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
